<!-- В файле README должно быть:
- описание задачи
- инструкция по запуску
- структура проекта
- что сделано в ноутбуке(этапы вашей работы)
- выводы

Пример README
Этапы analysis.ipynb:
- инициализация сессии
- загрузка датасета metadata.csv
- очистка данных(drop_duplicates()/fillna..)
- sql-запросы('select...'/какие/для чего)
- визуализация(heatmap/bar) -->

# Анализ медицинских данных

## Описание задачи
Разработка аналитической системы для эпидемиологического мониторинга COVID-19 на основе метаданных рентгеновских снимков с использованием PySpark и SQL-аналитики.

## Инструкция по запуску
1. Создать окружения: **macOS/Linux** - `python3 -m venv .venv`; **Windows** - `python -m venv .venv`
2. Установите зависимости: `pip install -r requirements.txt`
3. Установите Java на Linux: 
    ```bash
    sudo apt install openjdk-17-jdk-headless -y
    export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    echo "export JAVA_HOME=$JAVA_HOME" >> ~/.bashrc
    ```
2. Загрузите файл `metadata.csv` в корневую директорию проекта
3. Запустите Jupyter Notebook: `jupyter notebook analysis.ipynb`
4. Выполните ячейки последовательно

## Структура проекта
```
project/
├── analysis.ipynb          # Основной ноутбук с анализом
├── metadata.csv            # Исходные данные
├── processed_data/         # Результаты обработки (Parquet)
│   ├── df/                 # Основной обработанный датасет
│   ├── pneumonia_over_50/  # Пациенты с пневмонией >50 лет
│   └── complete_clinical_data/ # Полные клинические данные
└── README.md               # Документация
```

## Этапы работы в analysis.ipynb

### 1. Инициализация и загрузка данных
- Создание Spark сессии с оптимизациями
- Загрузка `metadata.csv` с парсингом дат в разных форматах
- Удаление лишнего столбца `_c29`

### 2. Предобработка данных
- Анализ пропусков по всем полям (survival - 62%, лабораторные показатели - 95-98%)
- Заполнение пропусков в поле `survival` на основе медицинских критериев
- Унификация диагнозов в категории: COVID-19, PNEUMONIA, NORMAL, OTHER
- Обработка возраста: фильтрация некорректных значений (0-120 лет)
- Удаление дубликатов

### 3. Анализ качества данных
- Построение распределения пропусков по всем полям
- Выявление аномалий в числовых полях (выбросы >3σ)
- Визуализация: bar chart (проценты пропусков) и boxplot (распределение значений)

### 4. SQL-аналитика
- **Запрос 1**: Базовая статистика по диагнозам (593 случая "Other", 27 "ToDo")
- **Запрос 2**: Распределение по полу и диагнозам (преобладание мужчин в "Other")
- **Запрос 3**: Топ-3 по возрасту в каждой группе диагнозов
- **Запрос 4**: Временные тренды (пик в 2020 году - 302 случая "Other")
- **Запрос 5**: Статистика по проекциям снимков (преобладание PA и AP)

### 5. Расширенная обработка в PySpark
- Создание UDF для категоризации возраста (Child, Adult, Senior и т.д.)
- Создание UDF для унификации диагнозов (COVID-19, Pneumonia, Tuberculosis и т.д.)
- Фильтрация данных:
  - Пневмония у пациентов >50 лет: 84 случая
  - Критические случаи COVID-19: 0 случаев
  - Полные клинические данные: 503 случая
- Сохранение результатов в формате Parquet с компрессией

## Выводы

1. **Качество данных**: Критические поля (survival, ICU статус, лабораторные показатели) имеют высокий процент пропусков (58-98%), что ограничивает анализ выживаемости.

2. **Распределение диагнозов**: Большинство случаев (593) классифицированы как "Other", что указывает на необходимость более детальной категоризации.

3. **Демография**: Преобладание мужчин (346 vs 186 женщин в категории "Other"), средний возраст пациентов - 53.5 года.

4. **Временные тренды**: Пик исследований в 2020 году (302 случая), что соответствует периоду пандемии COVID-19.

5. **Технические результаты**: Данные успешно обработаны, созданы структурированные датасеты для дальнейшего анализа, реализована полная цепочка ETL в PySpark.